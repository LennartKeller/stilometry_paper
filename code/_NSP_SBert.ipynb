{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d8861a-a608-466e-9694-6d7353f926e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300b1be5-f11b-4a5a-b886-093f3057d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from tqdm.auto import tqdm\n",
    "from nltk import sent_tokenize\n",
    "from more_itertools import windowed\n",
    "from random import choice, seed\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575499ed-fc1c-4cba-a778-1fd52ca77295",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"../data/rom_real_dataset_final.csv\")\n",
    "corpus = corpus[[\"author\", \"title\", \"epoch\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f89163-e45e-4f74-9ab1-1585523c76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = pd.concat([\n",
    "    corpus.query(\"epoch == 'romantik'\").sample(15, random_state=42),\n",
    "    corpus.query(\"epoch == 'realismus'\").sample(15, random_state=42)\n",
    "])\n",
    "corpus_test = corpus.drop(corpus_train.index)\n",
    "\n",
    "with open(\"../data/nsp_corpus_train_idx.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(map(str, corpus_train.index)))\n",
    "\n",
    "with open(\"../data/nsp_corpus_test_idx.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(map(str, corpus_test.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86317df0-c079-4e74-8999-0329dd52932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nsp_dataset(df, random_state=42):\n",
    "    seed(random_state)\n",
    "    data = []\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating NSP dataset\"):\n",
    "        text = row.pop(\"text\")\n",
    "        sents = sent_tokenize(text, language=\"german\")\n",
    "        for pair in windowed(sents, 2):\n",
    "            sent_1, sent_2 = pair \n",
    "            \n",
    "            true_pair = \" \".join(pair)\n",
    "            true_entry = row.copy()\n",
    "            true_entry[\"pair\"] = true_pair\n",
    "            true_entry[\"sent1\"] = sent_1\n",
    "            true_entry[\"sent2\"] = sent_2\n",
    "            true_entry[\"label\"] = 0\n",
    "            data.append(true_entry)\n",
    "            \n",
    "            # chose any sent from same text that is not sent_2\n",
    "            while (false_sent := choice(sents)) == sent_2:\n",
    "                pass\n",
    "            \n",
    "            false_pair = \" \".join((sent_1, false_sent))\n",
    "            false_entry = row.copy()\n",
    "            false_entry[\"pair\"] = false_pair\n",
    "            false_entry[\"sent1\"] = sent_1\n",
    "            false_entry[\"sent2\"] = false_sent\n",
    "            false_entry[\"label\"] = 1\n",
    "            data.append(false_entry)\n",
    "\n",
    "    return pd.DataFrame.from_records(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08801427-cffc-42e1-8943-11f6c20b549c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_dataset = make_nsp_dataset(corpus_train)\n",
    "#test_dataset = make_nsp_dataset(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb834893-9ac5-4f38-929e-20d78919d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.to_csv(\"../data/nsp_traindataset.csv\", index=False)\n",
    "#test_dataset.to_csv(\"../data/nsp_testdataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3e6a14-5c8d-4746-99dc-fa6832cb3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"../data/nsp_traindataset.csv\")\n",
    "test_dataset = pd.read_csv(\"../data/nsp_testdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c696365c-266d-4c12-a6af-b9dae327129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47136\n",
       "1    47134\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6bf58a-7d50-4dec-a86b-2e718389d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    381143\n",
       "1    381143\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239d1a9d-8075-42e1-b166-c418342f019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Create SBert Model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-german-cased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=384, activation_function=nn.Tanh())\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    modules=[word_embedding_model, pooling_model, dense_model],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83877aef-2c9e-4fda-9e49-d4916af2960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "LR = 3e-5\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 16\n",
    "\n",
    "WARMUP_STEPS = 250\n",
    "\n",
    "EVAL_STEPS = 2000\n",
    "\n",
    "OUTPUT_PATH = \"nsp_classif\"\n",
    "\n",
    "CPKT_PATH = \"nsp_classif/sbert\"\n",
    "SAVE_STEPS = 5000\n",
    "TOTAL_SAVE_LIMIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050b5ed2-b1a2-4b38-a1b8-122da3d107ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904607ce-ae40-461b-961e-39a468d9df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses, InputExample, SentencesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_sbert_dataset(dataset):\n",
    "    examples = [\n",
    "        InputExample(texts=[row[\"sent1\"], row[\"sent2\"]], label=row[\"label\"])\n",
    "        for _, row in dataset.iterrows()\n",
    "    ]\n",
    "    sbert_dataset = SentencesDataset(examples, model)\n",
    "    return sbert_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6057d8c7-2bc8-4dd1-b5b9-39cc1f9f6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(make_sbert_dataset(train_dataset), shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "val_dataloader = DataLoader(make_sbert_dataset(val_dataset), shuffle=True, batch_size=VAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2a5250-eb9d-41e0-b3bb-6ee8a0355a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate loss\n",
    "\n",
    "train_loss = losses.SoftmaxLoss(\n",
    "    model=model,\n",
    "    sentence_embedding_dimension=model.get_sentence_embedding_dimension(),\n",
    "    num_labels=2,\n",
    "    concatenation_sent_rep=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "475c83e9-bba0-4def-bded-4dc7c4af5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keller/.conda/envs/stilometry/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffebdd0450e431283ef638ba02811d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a57dfe4dc8e469b901503a8086d5b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961e62c372e74779b34f61e2c912513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db02f3d22550457cb7f28a6d64e1bd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2313396e7c24a9da5ebb5470d14646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aba2880c81f483cb1d4a45c96ae2914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    #evaluator=evaluator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    optimizer_params={\"lr\": LR},\n",
    "    #evaluation_steps=EVAL_STEPS,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    save_best_model=True,\n",
    "    checkpoint_path=CPKT_PATH,\n",
    "    checkpoint_save_steps=SAVE_STEPS,\n",
    "    checkpoint_save_total_limit=TOTAL_SAVE_LIMIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71160739-2de2-49ca-ac32-5ae9dbc77a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# the \"loss\" is actually a nn.module containing the weights of the classifier (and the actual model as submodule...)\n",
    "torch.save(train_loss, \"nsp_classif/train_loss_state.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba79735c-53ed-4fa5-a143-525ea3d07bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO morgen\n",
    "def predict(examples, softmax_loss):\n",
    "    true, pred = [], []\n",
    "    embeddings = []\n",
    "    for example in tqdm(examples, desc=\"Predicting...\"):\n",
    "        texts = example.texts\n",
    "        features = [\n",
    "            softmax_loss.model.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(softmax_loss.model.device)\n",
    "            for text in texts\n",
    "        ]\n",
    "        embs, output = softmax_loss(features, labels=None)\n",
    "        true.append(example.label)\n",
    "        pred.append(output.detach().cpu().numpy().reshape(-1).argmax())\n",
    "        embeddings.append(np.array([e.detach().cpu().numpy() for e in embs]))\n",
    "    return np.array(true), np.array(pred), embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1daa641d-48dc-470a-a2a9-adc86050070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b9c3296a94445ca4fa7993aba5d599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting...:   0%|          | 0/1886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true, pred, embeddings = predict(\n",
    "    make_sbert_dataset(val_dataset),\n",
    "    train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b820f2-d899-46fb-b1f8-8a40db87f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       941\n",
      "           1       0.68      0.69      0.69       945\n",
      "\n",
      "    accuracy                           0.68      1886\n",
      "   macro avg       0.68      0.68      0.68      1886\n",
      "weighted avg       0.68      0.68      0.68      1886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d5e4f-917f-49a4-acd9-b7c0c723cb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
